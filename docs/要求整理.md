---
tags:
  - dev
---
## 目的
- 特定のオーディオサンプル(以降Target Audioと記載)に類似する音色のシンセプリセットを検索する
## 背景
- 楽曲制作中に、サンプルパックなどのオーディオループサンプルで作業し始めて、編集したら、独自ののメロディにしたい時に、MIDIでシンセを鳴らす方法に切り替えるケースが多い
	- オーディオループサンプルでは演奏情報の編集ができないため
- 上記ケースでシンセのシンセの音色選びに時間がかかっている
- シンセプリセットのオーディオファイル(以降Preset Previewと記載)を所有している
## 検討
- Target Audioと音色が類似するPreset Previewを抽出することで目的を実現する
- 以下のようなフローで実現できると考えている
	1. \[事前準備\] Preset Previewの特徴データを計算して保存する
	2. Target Audioの特徴量データを計算する
	3. 1と2の類似度を比較し類似度の高いPreset Previewをランキングで出力する
- CLAP（Contrastive Language-Audio Pretraining）を仕様するのが良さそう
- 処理時間よりも音色の類似精度を優先する
- Pythonで実装する
- 初期はコアとなる処理部分を実装し、将来的にはGUIを持ったアプリケーションに発展させていきたい
- 類似度を判定するための特徴データはCLAPを検討しているが、他のライブラリや方式に変更しやすい構造で実装したい（PANN (CNN14) 、OpenL3他）
## 仕様
- Target Audioの仕様
	- wav形式
	- 44.1KHzまたは48KHz
	- 16bitまたは24bit
- Preset Previewの仕様
	- オーディオの内容は、そのプリセットをC3などのピッチでワンショットで鳴らしたもの
	- ogg形式
	- 約30000ファイル
## CLAPについての情報
### CLAPで実現できること

- **音色特徴量の高精度抽出**
    
    - 音声を 48 kHz モノラルのログメルスペクトログラムに変換し、Swin-Transformer 音声エンコーダから **512 次元の埋め込みベクトル**を得る。ベクトル空間では「質感・明るさ・テクスチャ」など語義的な音色属性が保たれるため、ピッチや演奏内容の差異を越えて“音色が近い”かどうかを比較できる。[huggingface.co](https://huggingface.co/docs/transformers/en/model_doc/clap?utm_source=chatgpt.com)
        
- **大規模サンプルライブラリの類似検索**
    
    1. 所有する30000ファイル（A群）の埋め込みを**オフラインで一括計算し保存**。
        
    2. 新規ワンショット（B）の埋め込みを生成。
        
    3. **コサイン類似度**で距離を計算し、距離が小さい順にランキング。
        
    4. 必要なら Faiss／HNSW などの近似最近傍インデックスを構築し、実行時の検索を高速化。  
        → 人が聴いて「質感が近い」と感じるサンプルが上位に並びやすく、音選びを大幅に効率化できる。[github.com](https://github.com/LAION-AI/CLAP?utm_source=chatgpt.com)
        
- **テキスト⇄音声のクロス検索**（オプション）
    
    - 同一空間にテキスト埋め込みも存在するため、「bright synth pluck」「warm pad」など**言語でのクエリ検索**や、自作タグによるライブラリ整理が可能。[huggingface.co](https://huggingface.co/docs/transformers/en/model_doc/clap?utm_source=chatgpt.com)
        
- **距離学習による精度ブースト**
    
    - ユーザーが作成する「似ている／似ていない」ペアを用いて、埋め込みの上に**Siamese ヘッドやコントラスト損失**を追加学習。
        
    - 既存の 512 次元ベクトルを入力に小規模ネットワークを訓練するだけでも効果があり、CLAP本体を凍結すれば計算コストを抑えられる。[github.com](https://github.com/LAION-AI/CLAP/issues/141?utm_source=chatgpt.com)
        

### 利用フロー（コードレス概要）

1. **環境準備**
    
    - Python + PyTorch + `transformers` または `laion-clap` をインストール。GPU 推奨（推論時約1 GB VRAM）。[huggingface.co](https://huggingface.co/docs/transformers/en/model_doc/clap?utm_source=chatgpt.com)
        
2. **モデル取得**
    
    - Hugging Face で公開されている `laion/music-audio-002` などのチェックポイントをロード。[huggingface.co](https://huggingface.co/docs/transformers/v4.46.2/ja/model_doc/clap?utm_source=chatgpt.com)
        
3. **前処理**
    
    - WAV を 48 kHz モノラルへ統一。
        
    - 長さは自動で平均プールされるのでトリミング不要（極端に短い場合は無音でパディング）。
        
4. **埋め込み生成**
    
    - モデルの `get_audio_features()` 相当 API に波形配列を渡し、512 次元ベクトルを取得。
        
5. **ベクトル保存**
    
    - NumPy/Parquet など軽量フォーマットでディスク保存。26 000 × 512 float32 ≒ 53 MB と小さく管理容易。
        
6. **検索実行**
    
    - クエリ B のベクトルと A 群ベクトルのコサイン距離を一括計算し、上位 N 件を返す。
        
    - 音源パスやメタ情報を紐づけておけば、そのまま DAW からプレビュー・ロードが可能。
        
7. **（任意）距離学習**
    
    - ラベル付きペアで Siamese ネットを訓練 → 距離重みやマッピングを更新 → 再ランキング。
        

### 運用のヒントと注意点

- **計算コスト**: 事前バッチ処理は GPU で数時間レベル。追加サンプルは随時バッチに追加すれば良い。
    
- **モデルサイズ**: オリジナル CLAP ≒ 400–600 MB、tiny 版もあり。ディスク／RAM は許容範囲。[pub.dega-akustik.de](https://pub.dega-akustik.de/DAS-DAGA_2025/files/upload/paper/149.pdf?utm_source=chatgpt.com)
    
- **48 kHz 仕様**: 入力が 44.1 kHz の場合はリサンプリングしても精度低下は軽微。
    
- **ライセンス**: **CC0-1.0** 公開で商用利用可（データセット側は別途確認）。[github.com](https://github.com/LAION-AI/CLAP?utm_source=chatgpt.com)
    
- **将来拡張**: モデル改良版や軽量蒸留版が継続的に公開されているため、埋め込み再計算の手順を自動化しておくと運用が楽。[pub.dega-akustik.de](https://pub.dega-akustik.de/DAS-DAGA_2025/files/upload/paper/149.pdf?utm_source=chatgpt.com)
